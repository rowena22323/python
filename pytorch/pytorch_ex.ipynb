{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597644845672",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2],[3,4]])\n",
    "x = torch.from_numpy(np.array([[1,2],[3,4]]))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "넘파이의 배열인 ndarray와 같은 개념입니다. 추가로 텐서간 연산에 따른 그래프와 경사도를 저장할 수 있습니다. \n",
    "파이토치는 텐서를 통해 값을 저장하고 그 값들에 대해 연산을 수행할 수 있는 함수를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1, 2],\n       [3, 4]])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "y = np.array([[1,2],[3,4]])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치는 자동으로 미분 및 역전파를 수행하는 autograd기능을 가진다.\n",
    "텐서들 간에 연산을 수행할 때마다 동적으로 연산그래프를 생성, 연산의 결과물이 어떤 텐서로부터 어떤 연산을 통해 왔는지 추적.\n",
    "\n",
    "최종적으로 나온 스칼라에 역전파 알고리즘을 통해 미분을 수행하도록 했을 때, 각 텐서는 자기자신의 자식노드에 해당하는 텐서와 연산을 자동으로 찾아 계속해서 역전파 알고리즘을 수행할 수 있도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.5414e-44, 0.0000e+00],\n        [1.4013e-45, 2.3511e-38]], grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "x1 = torch.FloatTensor(2,2)\n",
    "y1 = torch.FloatTensor(2,2)\n",
    "y1.requires_grad_(True)\n",
    "\n",
    "z = (x1+y1)+torch.FloatTensor(2,2)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd는 PyTorch에서 핵심적인 기능을 담당하는 하부 패키지이다. autograd는 텐서의 연산에 대해 자동으로 미분값을 구해주는 기능을 한다. 텐서 자료를 생성할 때, requires_grad인수를 True로 설정하거나 .requires_grad_(True)를 실행하면 그 텐서에 행해지는 모든 연산에 대한 미분값을 계산한다. 계산을 멈추고 싶으면 .detach()함수를 이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.5414e-44, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x2 = torch.FloatTensor(2,2)\n",
    "y2 = torch.FloatTensor(2,2)\n",
    "y2.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z1 = (x2+y2) + torch.FloatTensor(2,2)\n",
    "z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치로 1차원 벡터를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 1., 2., 3., 4., 5., 6.])\n1\ntorch.Size([7])\ntorch.Size([7])\n"
    }
   ],
   "source": [
    "t = torch.FloatTensor([0.,1.,2.,3.,4.,5.,6.])\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0.) tensor(1.) tensor(6.)\ntensor([2., 3., 4.]) tensor([4., 5.])\ntensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
    }
   ],
   "source": [
    "print(t[0], t[1], t[-1])  # 인덱스로 접근\n",
    "print(t[2:5], t[4:-1])    # 슬라이싱\n",
    "print(t[:2], t[3:])       # 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.],\n        [10., 11., 12.]])\n"
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\ntorch.Size([4, 3])\n"
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([ 2.,  5.,  8., 11.])\ntorch.Size([4])\n"
    }
   ],
   "source": [
    "print(t[:,1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져옴\n",
    "print(t[:,1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 1.,  2.],\n        [ 4.,  5.],\n        [ 7.,  8.],\n        [10., 11.]])\n"
    }
   ],
   "source": [
    "print(t[:,:-1]) # 맨 마지막 차원을 제외하고 다 가져오는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[5., 5.]])\n"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3,3]])\n",
    "m2 = torch.FloatTensor([[2,2]])\n",
    "print(m1+m2) # 크기가 동일한 텐서간의 덧셈연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[4., 5.]])\n"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1,2]])\n",
    "m2 = torch.FloatTensor([3]) #pytorch broadcasting [3] > [3,3]\n",
    "print(m1+m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[4., 5.],\n        [5., 6.]])\n"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1,2]])\n",
    "m2 = torch.FloatTensor([[3],[4]])\n",
    "print(m1+m2) # 벡터간 연산에서 브로드캐스팅이 적용되는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape of Matrix 1: torch.Size([2, 2])\nShape of Matrix 2: torch.Size([2, 1])\ntensor([[ 5.],\n        [11.]])\ntensor([[1., 2.],\n        [6., 8.]])\ntensor([[1., 2.],\n        [6., 8.]])\n"
    }
   ],
   "source": [
    "# 행렬 곱셈과 곱셈의 차이\n",
    "m1 = torch.FloatTensor([[1,2],[3,4]])\n",
    "m2 = torch.FloatTensor([[1],[2]])\n",
    "print('Shape of Matrix 1:', m1.shape)\n",
    "print('Shape of Matrix 2:', m2.shape)\n",
    "print(m1.matmul(m2))\n",
    "print(m1*m2)\n",
    "print(m1.mul(m2)) # element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(1.5000)\ntensor(2.5000)\n"
    }
   ],
   "source": [
    "t = torch.FloatTensor([1,2])\n",
    "print(t.mean())\n",
    "t1 = torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([2., 3.])\ntorch.Size([2])\ntensor([1.5000, 3.5000])\ntorch.Size([2])\n"
    }
   ],
   "source": [
    "print(t1.mean(dim=0))\n",
    "# t.mean(dim=0)은 입력에서 첫번째 차원을 제거한다. (1과 3의평균 2, 2와 4의 평균 3)\n",
    "mean1 = t1.mean(dim=0) # 행제거\n",
    "print(mean1.shape)\n",
    "\n",
    "mean2 = t1.mean(dim=1) # 열제거\n",
    "print(mean2)\n",
    "print(mean2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(10.)\ntensor([4., 6.])\ntensor([3., 7.])\ntensor([3., 7.])\n"
    }
   ],
   "source": [
    "print(t1.sum())\n",
    "print(t1.sum(dim=0))\n",
    "print(t1.sum(dim=1))\n",
    "print(t1.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(4.)\ntorch.return_types.max(\nvalues=tensor([3., 4.]),\nindices=tensor([1, 1]))\ntorch.return_types.max(\nvalues=tensor([2., 4.]),\nindices=tensor([1, 1]))\ntorch.return_types.max(\nvalues=tensor([2., 4.]),\nindices=tensor([1, 1]))\n"
    }
   ],
   "source": [
    "print(t1.max()) # 원소 중 최대값\n",
    "print(t1.max(dim=0)) # 첫번째 차원 제거\n",
    "print(t1.max(dim=1))\n",
    "print(t1.max(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Max :  tensor([3., 4.])\nArgmax :  tensor([1, 1])\n"
    }
   ],
   "source": [
    "# 두개를 함께 리턴받는 것이 아니라 max또는 argmax만 리턴받고 싶다면 리턴값에 인덱스를 부여\n",
    "\n",
    "print('Max : ', t1.max(dim=0)[0])\n",
    "print('Argmax : ', t1.max(dim=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인플레이스 연산 \n",
    "명령어 뒤에 '_'를 붙이면 자기자신의 값을 바꾸는 인플레이스 명령이 된다. 인플레이스 명령은 연산 결과를 반환하면서 동시에\n",
    "자기자신의 데이터를 고친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0, 1, 2, 3, 4])\ntensor([1, 3, 5, 7, 9])\ntensor([1, 3, 5, 7, 9])\n"
    }
   ],
   "source": [
    "x = torch.arange(0,5)\n",
    "z = torch.arange(1,6)\n",
    "\n",
    "print(x)\n",
    "print(x.add_(z))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View - 원소의 수를 유지하면서 텐서의 크기 변경\n",
    "- 파이토치 텐서의 뷰는 넘파이에서의 리쉐이프와 같은 역할 (텐서의 크기 변경)\n",
    "- view는 기본적으로 변경 전과 변경 후의 텐서 안의 원소의 개수가 유지되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[[0,1,2],\n",
    "                [3,4,5]],\n",
    "              [[6,7,8],\n",
    "                [9,10,11]]])\n",
    "ft = torch.FloatTensor(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 2, 3])\n"
    }
   ],
   "source": [
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\ntorch.Size([4, 3])\n"
    }
   ],
   "source": [
    "# 3차원 > 2차원 텐서로 변경\n",
    "\n",
    "print(ft.view([-1, 3])) #  (?,3)size의 2차원 텐서 생성\n",
    "print(ft.view([-1, 3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.,  1.,  2.]],\n\n        [[ 3.,  4.,  5.]],\n\n        [[ 6.,  7.,  8.]],\n\n        [[ 9., 10., 11.]]])\ntorch.Size([4, 1, 3])\n"
    }
   ],
   "source": [
    "# (2*2*3) = (?*1*3) = 12 를 만족하는 텐서 크기변경\n",
    "\n",
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squeeze / UnSqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.],\n        [1.],\n        [2.]])\ntorch.Size([3, 1])\n"
    }
   ],
   "source": [
    "ft1 = torch.FloatTensor([[0],[1],[2]])\n",
    "print(ft1)\n",
    "print(ft1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 1., 2.])\ntorch.Size([3])\n"
    }
   ],
   "source": [
    "print(ft1.squeeze()) # [3,1]이었던 두번째 차원이 제거되면서 [3,]의 크기를 갖는 텐서(1차원 벡터)로 변경\n",
    "print(ft1.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([3])\n"
    }
   ],
   "source": [
    "ft2 = torch.Tensor([0,1,2])\n",
    "print(ft2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 1., 2.]])\ntorch.Size([1, 3])\n"
    }
   ],
   "source": [
    "#Unsqueeze 연산\n",
    "print(ft2.unsqueeze(0))\n",
    "print(ft2.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 1., 2.]])\ntorch.Size([1, 3])\n"
    }
   ],
   "source": [
    "#View 연산 (== Unsqueeze)\n",
    "print(ft2.view(1, -1))\n",
    "print(ft2.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1, 2, 3, 4])\ntensor([1., 2., 3., 4.])\n"
    }
   ],
   "source": [
    "# 타입캐스팅\n",
    "\n",
    "lt = torch.LongTensor([1,2,3,4])\n",
    "print(lt)\n",
    "print(lt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1, 0, 0, 1], dtype=torch.uint8)\ntensor([1, 0, 0, 1])\ntensor([1., 0., 0., 1.])\n"
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt)\n",
    "print(bt.long()) # long type tensor\n",
    "print(bt.float()) # float type tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.],\n        [7., 8.]])\n"
    }
   ],
   "source": [
    "# concatenate\n",
    "# 딥러닝에서는 주로 모델의 입력 또는 중간 연산에서 두개의 텐서를 연결하는 경우가 많다. 두 텐서를 연결해서 입력으로 사용하는 것은 두가지의 정보를 모두 사용한다는 의미.\n",
    "x = torch.FloatTensor([[1,2],[3,4]])\n",
    "y = torch.FloatTensor([[5,6],[7,8]])\n",
    "print(torch.cat([x,y], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 2., 5., 6.],\n        [3., 4., 7., 8.]])\n"
    }
   ],
   "source": [
    "print(torch.cat([x,y], dim=1)) # 2*2 tensor > 2*4 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 4.],\n        [2., 5.],\n        [3., 6.]])\ntensor([[1., 4.],\n        [2., 5.],\n        [3., 6.]])\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\n"
    }
   ],
   "source": [
    "# stacking\n",
    "\n",
    "x = torch.FloatTensor([1,4])\n",
    "y = torch.FloatTensor([2,5])\n",
    "z = torch.FloatTensor([3,6])\n",
    "\n",
    "print(torch.stack([x,y,z]))\n",
    "\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))\n",
    "# unsqueeze(0)를 함으로써 3개의 벡터는 전부 (1,2)의 크기의 2차원 텐서로 변경.\n",
    "\n",
    "print(torch.stack([x,y,z], dim=1)) # 두번째 차원이 증가하도록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0., 1., 2.],\n        [2., 1., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n"
    }
   ],
   "source": [
    "# ones_like, zeros_like : 입력 텐서와 크기를 동일하게 하게 하면서 값을 1/0으로 채우기\n",
    "\n",
    "x = torch.FloatTensor([[0,1,2],[2,1,0]])\n",
    "print(x)\n",
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inplace Operation\n",
    "x = torch.FloatTensor([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 2.],\n        [3., 4.]])\ntensor([[2., 4.],\n        [6., 8.]])\ntensor([[2., 4.],\n        [6., 8.]])\n"
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.mul(2.))\n",
    "print(x.mul_(2.)) # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}