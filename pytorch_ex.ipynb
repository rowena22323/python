{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597479073588",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2],[3,4]])\n",
    "x = torch.from_numpy(np.array([[1,2],[3,4]]))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "넘파이의 배열인 ndarray와 같은 개념입니다. 추가로 텐서간 연산에 따른 그래프와 경사도를 저장할 수 있습니다. \n",
    "파이토치는 텐서를 통해 값을 저장하고 그 값들에 대해 연산을 수행할 수 있는 함수를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1, 2],\n       [3, 4]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "y = np.array([[1,2],[3,4]])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치는 자동으로 미분 및 역전파를 수행하는 autograd기능을 가진다.\n",
    "텐서들 간에 연산을 수행할 때마다 동적으로 연산그래프를 생성, 연산의 결과물이 어떤 텐서로부터 어떤 연산을 통해 왔는지 추적.\n",
    "\n",
    "최종적으로 나온 스칼라에 역전파 알고리즘을 통해 미분을 수행하도록 했을 때, 각 텐서는 자기자신의 자식노드에 해당하는 텐서와 연산을 자동으로 찾아 계속해서 역전파 알고리즘을 수행할 수 있도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.0000e+00, 0.0000e+00],\n        [5.4718e+22, 1.7282e-04]], grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "x1 = torch.FloatTensor(2,2)\n",
    "y1 = torch.FloatTensor(2,2)\n",
    "y1.requires_grad_(True)\n",
    "\n",
    "z = (x1+y1)+torch.FloatTensor(2,2)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd는 PyTorch에서 핵심적인 기능을 담당하는 하부 패키지이다. autograd는 텐서의 연산에 대해 자동으로 미분값을 구해주는 기능을 한다. 텐서 자료를 생성할 때, requires_grad인수를 True로 설정하거나 .requires_grad_(True)를 실행하면 그 텐서에 행해지는 모든 연산에 대한 미분값을 계산한다. 계산을 멈추고 싶으면 .detach()함수를 이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[8.4078e-45, 1.4433e-43],\n        [5.4718e+22, 1.7282e-04]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x2 = torch.FloatTensor(2,2)\n",
    "y2 = torch.FloatTensor(2,2)\n",
    "y.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z1 = (x2+y2) + torch.FloatTensor(2,2)\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "파이토치로 1차원 벡터를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0., 1., 2., 3., 4., 5., 6.])\n1\ntorch.Size([7])\ntorch.Size([7])\n"
    }
   ],
   "source": [
    "t = torch.FloatTensor([0.,1.,2.,3.,4.,5.,6.])\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(0.) tensor(1.) tensor(6.)\ntensor([2., 3., 4.]) tensor([4., 5.])\ntensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
    }
   ],
   "source": [
    "print(t[0], t[1], t[-1])  # 인덱스로 접근\n",
    "print(t[2:5], t[4:-1])    # 슬라이싱\n",
    "print(t[:2], t[3:])       # 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.],\n        [10., 11., 12.]])\n"
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\ntorch.Size([4, 3])\n"
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([ 2.,  5.,  8., 11.])\ntorch.Size([4])\n"
    }
   ],
   "source": [
    "print(t[:,1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져옴\n",
    "print(t[:,1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 1.,  2.],\n        [ 4.,  5.],\n        [ 7.,  8.],\n        [10., 11.]])\n"
    }
   ],
   "source": [
    "print(t[:,:-1]) # 맨 마지막 차원을 제외하고 다 가져오는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[5., 5.]])\n"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3,3]])\n",
    "m2 = torch.FloatTensor([[2,2]])\n",
    "print(m1+m2) # 크기가 동일한 텐서간의 덧셈연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[4., 5.]])\n"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1,2]])\n",
    "m2 = torch.FloatTensor([3]) #pytorch broadcasting [3] > [3,3]\n",
    "print(m1+m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[4., 5.],\n        [5., 6.]])\n"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1,2]])\n",
    "m2 = torch.FloatTensor([[3],[4]])\n",
    "print(m1+m2) # 벡터간 연산에서 브로드캐스팅이 적용되는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}